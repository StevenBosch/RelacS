The back-end will first convert the input sound file to sound energy values per frequency over time. This has not yet been implemented in the final product. To extract stressful sounds from the cochleograms, two machine learning techniques are used and their output is combined with a number of factors extracted from sound specific features.
The classifiers are trained on the raw energy values as well as on preprocessed values. These involved morphological filtering (erosion and dilation) and foreground-background filters with different time constants.

For the first technique the cochleogram is sliced into windows of about $\frac{3}{4}$ seconds that overlap for $50\%$. The probability that such a window contains a stressful sound is determined by looking solely at the mean intensity values per frequency band using a naive Bayes classifier.  

The second approach consist mainly of a convolutional neural network (CNN). For this approach, first, the cochleogram is preprocessed so that sounds that have a shorter duration than 1 second are supressed. Windows are extracted similarly. From these windows, features are extracted by the CNN. These are used as input for a multilayer perceptron, that outputs a the probability that the window is stressful. 

Both of these classifiers return a probability per window, which is subsequently averaged over the two. The resulting probability is then multiplied by two factors. 

The first of these factors is calculated as loudness feature. To get this factor first the average sound intensity per frame of the complete recording (so the $\approx$30 second file) is calculated as sum of all the frequency intensities in that frame. Then for every frame, the loudness factor is calculated as $\frac{I_f}{I_{avg}}$, in which $I_f$ is the frame intensity and $I_{avg}$ is the average intensity. The resulting factor thus becomes higher than 1 if the frame is louder than the average frame, and lower than 1 if it is more quiet. Finally per window the average over all its frames are is taken, giving the loudness factor for that window. As a result, if it is multiplied by its stress probability, this probability increases if the window is louder on average than the other windows in the recording.

The second factor is calculated in a similar way, but describes the suddenness of the sound. The transition between the intensities of two frames is compared to the average transition in the recording. If this is higher than average \textit{and} positive (in a negative transition the second frame is less loud than the first, making it not stressful), it is a more sudden than average sound. Again the average over the whole window is taken and multiplied with the probability, so that the probability of it being stress-inducing increases when it contains more than average sudden sounds.

Finally parts of sounds that have an extremely large amplitude with respect to the rest of a window ($>\mu + 2\sigma$) are also marked stressful (setting the probability at 1). This method ensures that, in case our classifier misclassifies a window that is in human eyes almost sure to be stress-inducing, the system will still give correct output. These methods are all combined into an array of stress level probabilities per window.  The output of the back-end is thus an array of stress levels per window.

%When the user makes a recording it is send to the back-end for processing. 
%The back-end will first convert the input to a HDF5
%file and do all the related processing. After this is done it will apply several
%machine learning methods. The results of all the machine learning methods are
%combined and a histogram of the stress levels in the file will be created based
%on the stress levels of each window in the input file.
